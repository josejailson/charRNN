{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3QzCHPMBkeb73hT3dI4Q1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josejailson/charRNN/blob/main/charRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ],
      "metadata": {
        "id": "pfAxH4PfZ8dF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKy7hyUgxTSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b090eadc-d30e-4200-fa55-571120bc38f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespeare_text[:80])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14dQbDdayQty",
        "outputId": "53242bbe-801c-4c72-ad3e-460a7dab6095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", standardize=\"lower\")\n",
        "text_vec_layer.adapt([shakespeare_text])\n",
        "encoded = text_vec_layer([shakespeare_text])[0]"
      ],
      "metadata": {
        "id": "AMLTW9_-yjIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded -= 2"
      ],
      "metadata": {
        "id": "KGT-QmcfzO35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens = text_vec_layer.vocabulary_size() - 2"
      ],
      "metadata": {
        "id": "FTEhm8BZ0DLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(encoded)"
      ],
      "metadata": {
        "id": "Aw8aUaEB0L1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the data"
      ],
      "metadata": {
        "id": "SAlqbzI3Z_QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
        "  ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "  ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
        "  ds = ds.flat_map(lambda window_ds: window_ds.batch(length+1))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ],
      "metadata": {
        "id": "TiznQbORbvsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O que to_dataset faz Ã© pegar o encoded text e criar um dataset contendo todas as windows com o tamanho desejado."
      ],
      "metadata": {
        "id": "rjgHOwEidgMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = 100\n",
        "tf.random.set_seed(42)\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42)\n",
        "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
        "test_set =  to_dataset(encoded[1_060_000:], length=length)"
      ],
      "metadata": {
        "id": "_eOw_ileaSyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building and training the Char-RNN Model"
      ],
      "metadata": {
        "id": "Jh75RJFkeBEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnPVb22qeAjl",
        "outputId": "fabfdd6d-95e4-41f7-e897-f18b7890d0fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  31247/Unknown - 1736s 54ms/step - loss: 1.4087 - accuracy: 0.5689"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31247/31247 [==============================] - 1790s 55ms/step - loss: 1.4087 - accuracy: 0.5689 - val_loss: 1.6171 - val_accuracy: 0.5295\n",
            "Epoch 2/10\n",
            "31247/31247 [==============================] - ETA: 0s - loss: 1.3092 - accuracy: 0.5918"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31247/31247 [==============================] - 1355s 42ms/step - loss: 1.3092 - accuracy: 0.5918 - val_loss: 1.5937 - val_accuracy: 0.5359\n",
            "Epoch 3/10\n",
            "31247/31247 [==============================] - ETA: 0s - loss: 1.2843 - accuracy: 0.5981"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31247/31247 [==============================] - 1300s 40ms/step - loss: 1.2843 - accuracy: 0.5981 - val_loss: 1.5817 - val_accuracy: 0.5408\n",
            "Epoch 4/10\n",
            "31247/31247 [==============================] - ETA: 0s - loss: 1.2702 - accuracy: 0.6017"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31247/31247 [==============================] - 1415s 44ms/step - loss: 1.2702 - accuracy: 0.6017 - val_loss: 1.5633 - val_accuracy: 0.5442\n",
            "Epoch 5/10\n",
            "31247/31247 [==============================] - ETA: 0s - loss: 1.2626 - accuracy: 0.6034"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31247/31247 [==============================] - 1975s 61ms/step - loss: 1.2626 - accuracy: 0.6034 - val_loss: 1.5651 - val_accuracy: 0.5463\n",
            "Epoch 6/10\n",
            "31247/31247 [==============================] - ETA: 0s - loss: 1.2574 - accuracy: 0.6045"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31247/31247 [==============================] - 1322s 41ms/step - loss: 1.2574 - accuracy: 0.6045 - val_loss: 1.5580 - val_accuracy: 0.5488\n",
            "Epoch 7/10\n",
            "31246/31247 [============================>.] - ETA: 0s - loss: 1.2538 - accuracy: 0.6054"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31247/31247 [==============================] - 1291s 40ms/step - loss: 1.2538 - accuracy: 0.6054 - val_loss: 1.5589 - val_accuracy: 0.5501\n",
            "Epoch 8/10\n",
            "31247/31247 [==============================] - 1356s 42ms/step - loss: 1.2508 - accuracy: 0.6063 - val_loss: 1.5625 - val_accuracy: 0.5477\n",
            "Epoch 9/10\n",
            "31247/31247 [==============================] - 1391s 43ms/step - loss: 1.2487 - accuracy: 0.6066 - val_loss: 1.5567 - val_accuracy: 0.5483\n",
            "Epoch 10/10\n",
            "31247/31247 [==============================] - 1453s 45ms/step - loss: 1.2460 - accuracy: 0.6074 - val_loss: 1.5500 - val_accuracy: 0.5485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X-2),\n",
        "    model\n",
        "])\n",
        "\n",
        "shakespeare_model.summary()"
      ],
      "metadata": {
        "id": "XnA5HzhDgO8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2c43a4-9a9a-4622-ac95-1fda6c737f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " lambda_3 (Lambda)           (None, None)              0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, None, 39)          61719     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,719\n",
            "Trainable params: 61,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ],
      "metadata": {
        "id": "b9Fv13R-gj4-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2338bc5c-8ea2-4ee0-a34b-a7a68a6a55c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 369ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/my_shakespeare_model/variables//"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf8yaWNcs7Hh",
        "outputId": "d0028f0c-4ce9-4b01-b8f2-bd07aa6ce6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "variables.data-00000-of-00001  variables.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_two = tf.keras.models.load_model(\"/content/models/my_shakespeare_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "Z_VJ-ODbu2mv",
        "outputId": "a32fd003-813a-4e6a-8e0b-6dfd88e57047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-e8c64fa42a13>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir -p ~/content/models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_two\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/models/my_shakespeare_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    231\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /content/models/my_shakespeare_model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model.save(\"sample_data/my_shakespeare_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkPe4l8PsTir",
        "outputId": "42e616a4-22c4-4304-a555-dc388f84c247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating fake Shakesperean text"
      ],
      "metadata": {
        "id": "gD39d3lyhk1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probas = tf.math.log([[0.5, 0.4, 0.1]])\n",
        "tf.random.set_seed(42)\n",
        "tf.random.categorical(log_probas, num_samples=8)"
      ],
      "metadata": {
        "id": "nnMq9qhVhkLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "  y_proba = shakespeare_model.predict([text])[0, -1:]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0,0]\n",
        "  return text_vec_layer.get_vocabulary()[char_id + 2]"
      ],
      "metadata": {
        "id": "iNR9MTMfiEUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extended_text(text, n_chars=50, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "metadata": {
        "id": "EYkGtzrgjoSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "print(extended_text(\"To be or not to be\"), temperature=0.01)"
      ],
      "metadata": {
        "id": "_hjKoLJ8kCI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extended_text(\"To be or not to be\"), temperature=1)"
      ],
      "metadata": {
        "id": "CkDXAlAckOBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extended_text(\"To be or not to be\"), temperature=1)"
      ],
      "metadata": {
        "id": "zjLTUQLfkXq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stateful RNN"
      ],
      "metadata": {
        "id": "02n60bjzkrct"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9r4mF7CwkrI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}